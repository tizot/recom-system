{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import MySQLdb\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne import layers, init, nonlinearities\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "#from dssm import build_multi_dssm\n",
    "from dataset_tools import *\n",
    "from dssm import build_multi_dssm\n",
    "from string_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DSSM params and user's papers (raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = np.load('../data/datasets/dataset-pasi.npz')\n",
    "ngrams = loader['ngrams']\n",
    "num_entries = loader['num_entries'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list2paper(l, r_index=None, r_author=None, r_title=None, r_abstract=None, r_cite=None):\n",
    "    \"\"\"\n",
    "    Transform a raw data paper formatted as a list into dict\n",
    "    \"\"\"\n",
    "    p = {'index': None, 'authors': [], 'title': None, 'abstract': None, 'citations': []}\n",
    "    \n",
    "    if r_index is None:\n",
    "        r_index = re.compile('^#index(.*)')\n",
    "    if r_author is None:\n",
    "        r_author = re.compile('^#@(.*)')\n",
    "    if r_title is None:\n",
    "        r_title = re.compile('^#\\*(.*)')\n",
    "    if r_abstract is None:\n",
    "        r_abstract = re.compile('^#!(.*)')\n",
    "    if r_cite is None:\n",
    "        r_cite = re.compile('^#%(.*)')\n",
    "        \n",
    "    for s in l:\n",
    "        m_index = r_index.match(s)\n",
    "        if m_index is not None:\n",
    "            p['index'] = m_index.group(1)\n",
    "        \n",
    "        m_author = r_author.match(s)\n",
    "        if m_author is not None:\n",
    "            p['authors'] = [a.strip() for a in m_author.group(1).split(',')]\n",
    "        \n",
    "        m_title = r_title.match(s)\n",
    "        if m_title is not None:\n",
    "            p['title'] = m_title.group(1)\n",
    "        \n",
    "        m_abstract = r_abstract.match(s)\n",
    "        if m_abstract is not None:\n",
    "            p['abstract'] = m_abstract.group(1)\n",
    "        \n",
    "        m_cite = r_cite.match(s)\n",
    "        if m_cite is not None:\n",
    "            p['citations'].append(m_cite.group(1))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_papers(input_file, author_name):\n",
    "    \"\"\"\n",
    "    Read input file and return a list of dicts\n",
    "    \"\"\"\n",
    "    r_index = re.compile('^#index(.*)')\n",
    "    r_author = re.compile('^#@(.*)')\n",
    "    r_title = re.compile('^#\\*(.*)')\n",
    "    r_abstract = re.compile('^#!(.*)')\n",
    "    r_cite = re.compile('^#%(.*)')\n",
    "\n",
    "    # Split result into a list of lists (each sublist is a paper)\n",
    "    papers = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        p = []\n",
    "        for l in content:\n",
    "            if l.strip() != '':\n",
    "                p.append(l)\n",
    "            else:\n",
    "                papers.append(p)\n",
    "                p = [] \n",
    "    \n",
    "    papers = [list2paper(l) for l in papers]\n",
    "    \n",
    "    author_papers = []\n",
    "    papers_without_abstract = []\n",
    "    \n",
    "    for p in papers:\n",
    "        if author_name in p['authors']:\n",
    "            if p['abstract'] is not None:\n",
    "                author_papers.append(p)\n",
    "            else:\n",
    "                papers_without_abstract.append(p)\n",
    "    \n",
    "    return author_papers, papers_without_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_papers_from_raw(user_papers, ngrams=None, verbosity=1):\n",
    "    \"\"\"\n",
    "    Compute the user's papers features\n",
    "    \"\"\"\n",
    "    sh = StringHasher()\n",
    "    sc = StringCleaner()\n",
    "    \n",
    "    if ngrams is None:\n",
    "        # Generate author's vocabulary\n",
    "        tokens = generate_user_vocab(user_papers)\n",
    "\n",
    "        # Initiate the ngrams (specific to the author)\n",
    "        sh.init_ngrams(tokens)\n",
    "    else:\n",
    "        sh.load_ngrams(ngrams)\n",
    "    \n",
    "    # Hash user's papers' titles and abstracts\n",
    "    papers_feat = {}\n",
    "    total = len(user_papers)\n",
    "    i = 1\n",
    "    start_time = time.time()\n",
    "    for p in user_papers:\n",
    "        title = sc.clean_string(p['title'])\n",
    "        abstract = sc.clean_string(p['abstract'])\n",
    "        to_hash = title + \" \" + abstract\n",
    "        papers_feat[p['index']] = sh.hash(to_hash)\n",
    "        if verbosity > 1 and i % 100 == 0:\n",
    "            print(\"Paper %d over %d\" % (i, total))\n",
    "        i += 1\n",
    "    \n",
    "    return papers_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_papers_raw, _ = retrieve_papers('../data/pasi_papers.txt', 'Gabriella Pasi')\n",
    "\n",
    "papers_feat = user_papers_from_raw(user_papers_raw, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DSSM, load params and compute user's papers projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dssm_struct is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ed9e254575a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdssm_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/outputs/output-pasi.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdssm_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dssm_struct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnum_hid1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdssm_struct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_hid1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_hid2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdssm_struct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_hid2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/camille/Projects/Unimib/recom-diff/.env/lib/python3.4/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not a file in the archive\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dssm_struct is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "num_samples = 1\n",
    "dssm_loader = np.load('../data/outputs/output-pasi.npz')\n",
    "dssm_struct = loader\n",
    "num_hid1 = dssm_struct['num_hid1']\n",
    "num_hid2 = dssm_struct['num_hid2']\n",
    "num_out = dssm_struct['num_out']\n",
    "gamma = 500.0 # dssm_struct['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.matrix()\n",
    "dssm_values = dssm_loader['dssm']\n",
    "network = build_multi_dssm(input_var=input_var, \n",
    "                                      num_samples=num_samples, \n",
    "                                      num_entries=num_entries,\n",
    "                                      num_ngrams=len(ngrams), \n",
    "                                      num_hid1=num_hid1, \n",
    "                                      num_hid2=num_hid2, \n",
    "                                      num_out=num_out)\n",
    "lasagne.layers.set_all_param_values(network, dssm_values)\n",
    "prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "output = prediction / prediction.norm(L=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = theano.function([input_var], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_papers = [f(x.reshape(1, -1))[0] for _, x in papers_feat.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores for \"new\" papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_papers = [\n",
    "    {\n",
    "        'title': \"Statistical Language Models for Information Retrieval A Critical Review\",\n",
    "        'abstract': 'Abstract Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling nontraditional retrieval problems. In general, statistical language models provide a principled way of modeling various kinds of retrieval problems. The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point out outstanding challenges.',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Personalized information delivery: an analysis of information filtering methods',\n",
    "        'abstract': 'With the increasing availability of information in electronic form, it becomes more important and feasible to have automatic methods to filter information. Research organizations generate large amounts of information, which can include departmental and technical memoranda, announcements of meetings and conferences, and minutes from meetings. This volume of information makes it difficult to keep employees apprised of all relevant work. Furthermore, only a small fraction of the available information will actually be relevant to any particular employee within an organization that covers a variety of areas. Thus, there is the problem of determining what information is of interest to the employee, while minimizing the amount of search through irrelevant information. This research tested several information-retrieval methods for filtering technical memos. Filtering of information is not a new concept, nor is it one that is limited to electronic documents. When we read standard paper texts, information filtering occurs. We only buy certain magazines, since other magazines may contain information that is redundant or irrelevant to our interests. In this way, we are filtering out some of the large amount of information to which we have access. Within any particular magazine, we also choose articles that appear relevant to our interests. Thus, when people are engaged in any sort of acquisition of information, they continually filter information. With the advent of electronic presentation of information, some of that filtering need no longer be done by us, but could be done automatically by the system that presents the information.',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Personalizing search via automated analysis of interests and activities',\n",
    "        'abstract': 'We formulate and study search algorithms that consider a user\\'s prior interactions with a wide variety of content to personalize that user\\'s current Web search. Rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user\\'s interests. This information is used to re-rank Web search results within a relevance feedback framework. We explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited Web pages, and other information about the user such as documents and email the user has read and created. Our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. We show that such personalization algorithms can significantly improve on current Web search.',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Information retrieval based on fuzzy associations',\n",
    "        'abstract': 'The aim of the present paper is to propose a fuzzy set model for information retrieval and to develop methods and algorithms for fuzzy information retrieval based on the fuzzy set model. A process of information retrieval is represented as a diagram that consists of three components. Each component has its inherent fuzziness. As typical examples for describing the three components, we consider a fuzzy association as a generalization of a fuzzy thesaurus for the first component, a fuzzy inverted index for the second component, and a fuzzy filter for the third component. Efficient algorithms for fuzzy retrieval on large scale bibliographic databases are developed. The significance of the present method is that current techniques in researches of bibliographic databases without fuzzy sets are studied in the framework of fuzzy sets and their implications are made clear using the model herein.',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Fuzzy information systems: managing uncertainty in databases and information retrieval systems',\n",
    "        'abstract': 'Querying for information is a commonality between databases and information retrieval systems. For both areas, there are a variety of issues relative to representation of uncertainty and its retrieval. This paper reviews these issues for both types of systems and discusses potential future directions.',\n",
    "    },\n",
    "]\n",
    "\n",
    "bad_papers = [\n",
    "    {\n",
    "        'title': \"Neural-network-based fuzzy logic control and decision system\",\n",
    "        'abstract': \"A general neural-network (connectionist) model for fuzzy logic control and decision systems is proposed. This connectionist model, in the form of feedforward multilayer net, combines the idea of fuzzy logic controller and neural-network structure and learning abilities into an integrated neural-network-based fuzzy logic control and decision system. A fuzzy logic control decision network is constructed automatically by learning the training examples itself. By combining both unsupervised (self-organized) and supervised learning schemes, the learning speed converges much faster than the original backpropagation learning algorithm. The connectionist structure avoids the rule-matching time of the inference engine in the traditional fuzzy logic system. Two examples are presented to illustrate the performance and applicability of the proposed model.\"\n",
    "    },\n",
    "    {\n",
    "        'title': \"A hierarchical type-2 fuzzy logic control architecture for autonomous mobile robots\",\n",
    "        'abstract': \"Autonomous mobile robots navigating in changing and dynamic unstructured environments like the outdoor environments need to cope with large amounts of uncertainties that are inherent of natural environments. The traditional type-1 fuzzy logic controller (FLC) using precise type-1 fuzzy sets cannot fully handle such uncertainties. A type-2 FLC using type-2 fuzzy sets can handle such uncertainties to produce a better performance. In this paper, we present a novel reactive control architecture for autonomous mobile robots that is based on type-2 FLC to implement the basic navigation behaviors and the coordination between these behaviors to produce a type-2 hierarchical FLC. In our experiments, we implemented this type-2 architecture in different types of mobile robots navigating in indoor and outdoor unstructured and challenging environments. The type-2-based control system dealt with the uncertainties facing mobile robots in unstructured environments and resulted in a very good performance that outperformed the type-1-based control system while achieving a significant rule reduction compared to the type-1 system.\"\n",
    "    },\n",
    "    {\n",
    "        'title': \"Data mining in bioinformatics using Weka\",\n",
    "        'abstract': \"Summary: The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection—common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.\"\n",
    "    },\n",
    "    {\n",
    "        'title': \"A comparison of normalization methods for high density oligonucleotide array data based on variance and bias\",\n",
    "        'abstract': \"Motivation: When running experiments that involve multiple high density oligonucleotide arrays, it is important to remove sources of variation between arrays of non-biological origin. Normalization is a process for reducing this variation. It is common to see non-linear relations between arrays and the standard normalization provided by Affymetrix does not perform well in these situations. Results: We present three methods of performing normalization at the probe intensity level. These methods are called complete data methods because they make use of data from all arrays in an experiment to form the normalizing relation. These algorithms are compared to two methods that make use of a baseline array: a one number scaling based algorithm and a method that uses a non-linear normalizing relation by comparing the variability and bias of an expression measure. Two publicly available datasets are used to carry out the comparisons. The simplest and quickest complete data method is found to perform favorably. Availability: Software implementing all three of the complete data normalization methods is available as part of the R package Affy, which is a part of the Bioconductor project http://www.bioconductor.org.\"\n",
    "    },\n",
    "    {\n",
    "        'title': \"MEGA: Molecular evolutionary genetics analysis software for microcomputers\",\n",
    "        'abstract': \"A computer program package called MEGA has been developed for estimating evolutionary distances, reconstructing phylogenetic trees and computing basic statistical quantities from molecular data. It is written in C++ and is intended to be used on IBM and IBM-compatible personal computers. In this program, various methods for estimating evolutionary distances from nucleotide and amino acid sequence data, three different methods of phylogenetic inference (UPGMA, neighborjoining and maximum parsimony) and two statistical tests of topological differences are included. For the maximum parsimony method, new algorithms of branch-and-bound and heuristic searches are implemented. In addition, MEGA computes statistical quantities such as nucleotide and amino acid frequencies, transition/transversion biases, codon frequencies (codon usage tables), and the number of variable sites in specified segments in nucleotide and amino acid sequences. Advanced on-screen sequence data and phylogenetictree editors facilitate publication-quality outputs with a wide range of printers. Integrated and interactive designs, on-line context-sensitive helps, and a text-file editor make MEGA easy to use. © 1994 Oxford University Press.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hash_paper(p_title, p_abstract, ngrams):\n",
    "    sh = StringHasher()\n",
    "    sc = StringCleaner()\n",
    "    \n",
    "    sh.load_ngrams(ngrams)\n",
    "    \n",
    "    title = sc.clean_string(p_title)\n",
    "    abstract = sc.clean_string(p_abstract)\n",
    "    to_hash = title + \" \" + abstract\n",
    "    p_feats = sh.hash(to_hash)\n",
    "    \n",
    "    return p_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarities(paper, user_papers):\n",
    "    return np.array([np.dot(paper, y)[0] for y in user_papers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probas(paper, user_papers, gamma=500.0):\n",
    "    similarities = get_similarities(paper, user_papers)\n",
    "    \n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    return softmax(gamma * similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def results(papers):\n",
    "    sims, sims_d = [], []\n",
    "    for i, p in enumerate(papers):\n",
    "        paper = hash_paper(p['title'], p['abstract'], ngrams)\n",
    "        paper = f(paper.reshape(1, -1))\n",
    "        sim = pd.Series(get_similarities(paper, user_papers))\n",
    "        sims.append(sim)\n",
    "        sims_d.append(sim.describe())\n",
    "    return sims, sims_d\n",
    "\n",
    "goods, goods_d = results(good_papers)\n",
    "bads, bads_d = results(bad_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp = bad_papers[2]\n",
    "gp = hash_paper(gp['title'], gp['abstract'], ngrams)\n",
    "gp = f(gp.reshape(1, -1))\n",
    "sims = get_similarities(gp, user_papers)\n",
    "sorted(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.plot(sorted(goods[i]))\n",
    "plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(sorted(bads[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
